{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre-trained MobileNetV2 as the encoder + UNet decoder\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Recall, Precision, MeanIoU\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read one sample from image and mask to get the size of pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512\n",
    "EPOCHS = 50\n",
    "BATCH = 8\n",
    "LR = 1e-4\n",
    "PATH = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and split data into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, split=0.1):\n",
    "    images = sorted(glob(os.path.join(path, \"image/*\")))\n",
    "    masks = sorted(glob(os.path.join(path, \"mask/*\")))\n",
    "\n",
    "    total_size = len(images)\n",
    "    valid_size = int(split * total_size)\n",
    "    test_size = int(split * total_size)\n",
    "\n",
    "    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=0)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=0)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=0)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=0)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read images and masks, resize to 256 * 256 and normalize. The original is 224*224*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    x = x/255.0\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    x = x/255.0\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tow function below help to build data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])\n",
    "    x.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    y.set_shape([IMAGE_SIZE, IMAGE_SIZE, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(x, y, batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model function builds the architecture for encoder+decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\n",
    "    \n",
    "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=1.0)\n",
    "    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    \n",
    "    f = [16, 32, 48, 64]\n",
    "    x = encoder_output\n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dice coefficient measuers performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1e-15\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(PATH, split=0.1)\n",
    "train_dataset = tf_dataset(train_x, train_y, batch=BATCH)\n",
    "valid_dataset = tf_dataset(valid_x, valid_y, batch=BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, adam optimizer with dice coefficient loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junh/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = model()\n",
    "opt = tf.keras.optimizers.Adam(LR)\n",
    "metrics = [MeanIoU(num_classes=2), Recall(), Precision()]\n",
    "model.compile(loss=dice_loss, optimizer=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two callbacks help to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=6),\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(train_x)//BATCH\n",
    "valid_steps = len(valid_x)//BATCH\n",
    "\n",
    "if len(train_x) % BATCH != 0:\n",
    "    train_steps += 1\n",
    "if len(valid_x) % BATCH != 0:\n",
    "    valid_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 24 steps, validate for 3 steps\n",
      "Epoch 1/50\n",
      "24/24 [==============================] - 308s 13s/step - loss: 0.6688 - mean_io_u_1: 0.3944 - recall_1: 0.5327 - precision_1: 0.3251 - val_loss: 0.7556 - val_mean_io_u_1: 0.4027 - val_recall_1: 2.4094e-04 - val_precision_1: 0.1023\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 295s 12s/step - loss: 0.5465 - mean_io_u_1: 0.3944 - recall_1: 0.9169 - precision_1: 0.4672 - val_loss: 0.7015 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.5553 - val_precision_1: 0.2633\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.4481 - mean_io_u_1: 0.3944 - recall_1: 0.9557 - precision_1: 0.6231 - val_loss: 0.6802 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.9992 - val_precision_1: 0.2413\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.3695 - mean_io_u_1: 0.3944 - recall_1: 0.9663 - precision_1: 0.7401 - val_loss: 0.6173 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.9992 - val_precision_1: 0.2420\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.3206 - mean_io_u_1: 0.3944 - recall_1: 0.9726 - precision_1: 0.8088 - val_loss: 0.5844 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.9947 - val_precision_1: 0.2954\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.2931 - mean_io_u_1: 0.3944 - recall_1: 0.9778 - precision_1: 0.8506 - val_loss: 0.5673 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.9712 - val_precision_1: 0.3580\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.2735 - mean_io_u_1: 0.3944 - recall_1: 0.9795 - precision_1: 0.8822 - val_loss: 0.5518 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.8905 - val_precision_1: 0.4524\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.2574 - mean_io_u_1: 0.3944 - recall_1: 0.9819 - precision_1: 0.9033 - val_loss: 0.5414 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.8383 - val_precision_1: 0.5035\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.2463 - mean_io_u_1: 0.3944 - recall_1: 0.9808 - precision_1: 0.9158 - val_loss: 0.5445 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.7177 - val_precision_1: 0.5815\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 295s 12s/step - loss: 0.2367 - mean_io_u_1: 0.3944 - recall_1: 0.9807 - precision_1: 0.9268 - val_loss: 0.4997 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.8747 - val_precision_1: 0.5234\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.2284 - mean_io_u_1: 0.3944 - recall_1: 0.9811 - precision_1: 0.9324 - val_loss: 0.4854 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.7471 - val_precision_1: 0.6572\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.2239 - mean_io_u_1: 0.3944 - recall_1: 0.9783 - precision_1: 0.9329 - val_loss: 0.4402 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.8950 - val_precision_1: 0.5942\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 295s 12s/step - loss: 0.2171 - mean_io_u_1: 0.3944 - recall_1: 0.9820 - precision_1: 0.9320 - val_loss: 0.4669 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.7216 - val_precision_1: 0.6589\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 295s 12s/step - loss: 0.2040 - mean_io_u_1: 0.3944 - recall_1: 0.9878 - precision_1: 0.9547 - val_loss: 0.5408 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4763 - val_precision_1: 0.7501\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1958 - mean_io_u_1: 0.3944 - recall_1: 0.9896 - precision_1: 0.9649 - val_loss: 0.5557 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4248 - val_precision_1: 0.7708\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 295s 12s/step - loss: 0.1893 - mean_io_u_1: 0.3944 - recall_1: 0.9912 - precision_1: 0.9689 - val_loss: 0.5833 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.3613 - val_precision_1: 0.7969\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1833 - mean_io_u_1: 0.3944 - recall_1: 0.9921 - precision_1: 0.9726 - val_loss: 0.5495 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4103 - val_precision_1: 0.8053\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 295s 12s/step - loss: 0.1780 - mean_io_u_1: 0.3944 - recall_1: 0.9928 - precision_1: 0.9750 - val_loss: 0.5426 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4133 - val_precision_1: 0.8098\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1748 - mean_io_u_1: 0.3944 - recall_1: 0.9935 - precision_1: 0.9758 - val_loss: 0.5450 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4014 - val_precision_1: 0.8139\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1741 - mean_io_u_1: 0.3944 - recall_1: 0.9936 - precision_1: 0.9770 - val_loss: 0.5476 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.3920 - val_precision_1: 0.8200\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1735 - mean_io_u_1: 0.3944 - recall_1: 0.9936 - precision_1: 0.9777 - val_loss: 0.5426 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.3960 - val_precision_1: 0.8213\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1730 - mean_io_u_1: 0.3944 - recall_1: 0.9936 - precision_1: 0.9780 - val_loss: 0.5353 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4046 - val_precision_1: 0.8210\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1724 - mean_io_u_1: 0.3944 - recall_1: 0.9937 - precision_1: 0.9784 - val_loss: 0.5281 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4132 - val_precision_1: 0.8210\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1719 - mean_io_u_1: 0.3944 - recall_1: 0.9937 - precision_1: 0.9787 - val_loss: 0.5208 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4226 - val_precision_1: 0.8211\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1715 - mean_io_u_1: 0.3944 - recall_1: 0.9938 - precision_1: 0.9791 - val_loss: 0.5076 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4428 - val_precision_1: 0.8186\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1714 - mean_io_u_1: 0.3944 - recall_1: 0.9938 - precision_1: 0.9791 - val_loss: 0.4952 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4625 - val_precision_1: 0.8170\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 294s 12s/step - loss: 0.1714 - mean_io_u_1: 0.3944 - recall_1: 0.9938 - precision_1: 0.9792 - val_loss: 0.4838 - val_mean_io_u_1: 0.4027 - val_recall_1: 0.4802 - val_precision_1: 0.8160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbc7bb87a50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 9s 3s/step - loss: 0.4508 - mean_io_u: 0.3870 - recall: 0.9871 - precision: 0.5052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.45082863171895343, 0.38695064, 0.98707616, 0.5051604]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = tf_dataset(test_x, test_y, batch=BATCH)\n",
    "\n",
    "test_steps = (len(test_x)//BATCH)\n",
    "if len(test_x) % BATCH != 0:\n",
    "    test_steps += 1\n",
    "\n",
    "model.evaluate(test_dataset, steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"src/models1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models.h5',custom_objects={'dice_loss': dice_loss, 'dice_coef':dice_coef})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    x = x/255.0\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x = x/255.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(read_image(train_x[0]), axis=0))[0]>0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
